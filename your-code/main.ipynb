{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Challenge\n",
    "## Context\n",
    "You work in the data analysis team of a very important company. On Monday, the company shares some good news with you: you just got hired by a major retail company! So, let's get prepared for a huge amount of work!\n",
    "\n",
    "Then you get to work with your team and define the following tasks to perform:   \n",
    "1. You need to start your analysis using data from the past.  \n",
    "2. You need to define a process that takes your daily data as an input and integrates it.  \n",
    "\n",
    "You are in charge of the second part, so you are provided with a sample file that you will have to read daily. To complete you task, you need the following aggregates:\n",
    "* One aggregate per store that adds up the rest of the values.\n",
    "* One aggregate per item that adds up the rest of the values.\n",
    "\n",
    "You can import the dataset `retail_sales` from Ironhack's database. \n",
    "\n",
    "## Your task\n",
    "Therefore, your process will consist of the following steps:\n",
    "1. Read the sample file that a daily process will save in your folder. \n",
    "2. Clean up the data.\n",
    "3. Create the aggregates.\n",
    "4. Write three tables in your local database: \n",
    "    - A table for the cleaned data.\n",
    "    - A table for the aggregate per store.\n",
    "    - A table for the aggregate per item.\n",
    "\n",
    "## Instructions\n",
    "* Read the csv you can find in Ironhack's database.\n",
    "* Clean the data and create the aggregates as you consider.\n",
    "* Create the tables in your local database.\n",
    "* Populate them with your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the workspace and import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miner\\Anaconda3\\lib\\site-packages\\pymysql\\cursors.py:170: Warning: (1366, \"Incorrect string value: '\\\\xE1ndar ...' for column 'VARIABLE_VALUE' at row 1\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "## Import libraries\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "## Connect to DB\n",
    "# driver   = 'mysql+pymysql:'\n",
    "# user     = 'data-students' \n",
    "# password = 'iR0nH@cK-D4T4B4S3'\n",
    "# ip       = '34.65.10.136' \n",
    "# database = 'retail_sales' \n",
    "\n",
    "\n",
    "driver   = 'mysql+pymysql:'\n",
    "user     = 'root' \n",
    "password = 'ML_LI_04'\n",
    "ip       = '127.0.0.1' \n",
    "database = 'retail_sales' \n",
    "\n",
    "connection_string = f'{driver}//{user}:{password}@{ip}/{database}'\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "## Import day data\n",
    "\n",
    "raw_sales_original = pd.read_sql('SELECT * FROM raw_sales;', engine)\n",
    "raw_sales          = pd.read_sql('SELECT * FROM raw_sales;', engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data headers (data relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  shop_id  item_id  item_price  item_cnt_day\n",
      "0 2015-01-04       29     1469      1199.0           1.0\n",
      "1 2015-01-04       28    21364       479.0           1.0\n",
      "2 2015-01-04       28    21365       999.0           2.0\n",
      "3 2015-01-04       28    22104       249.0           2.0\n"
     ]
    }
   ],
   "source": [
    "print(raw_sales.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provided in the dataset is all relevant to the aggregates we want to create. Therefore I would not create a subset of\n",
    "data to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data structure (NaN values, types of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4545 entries, 0 to 4544\n",
      "Data columns (total 5 columns):\n",
      "date            4545 non-null datetime64[ns]\n",
      "shop_id         4545 non-null int64\n",
      "item_id         4545 non-null int64\n",
      "item_price      4545 non-null float64\n",
      "item_cnt_day    4545 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(2)\n",
      "memory usage: 177.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(raw_sales.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NaN values:** the current dataset contains no NaN. If there were rows with NaN values, and assuming that retrieving the\n",
    "information again from the shop system would require time and effort that our client does not wish to invest, I will \n",
    "proceed as follows:\n",
    "\n",
    "* For date: Since we are receiving daily values, I would assume that I can infer the value from the rest of the values in\n",
    "the column.\n",
    "* For shop_id: I won't discard the row as the info is partially valid (i.e: it can provide valuable info on the item aggregate table) Therefore I will replace NaN by 0 (assuming no shop_id = 0)\n",
    "\n",
    "* For item_id: I won't discard the row as the info is partially valid (i.e: it can provide valuable info on the shop aggregate table) Therefore I will replace NaN by 0 (assuming no item_id = 0)\n",
    "\n",
    "* For item_price: As the price can't be infered, I would discard the record. However, I will provide a warning message so measures can be taken if considered necessary.\n",
    "\n",
    "* For item_cnt_day: As the sales per day can't be infered, I would discard the record. However, I will provide a warning message so measures can be taken if considered necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  shop_id  item_id  item_price  item_cnt_day\n",
      "0    2015-01-04       29     1469      1199.0           1.0\n",
      "1    2015-01-04       28    21364       479.0           1.0\n",
      "2    2015-01-04       28    21365       999.0           2.0\n",
      "3    2015-01-04       28    22104       249.0           2.0\n",
      "4    2015-01-04       28    22091       179.0           1.0\n",
      "...         ...      ...      ...         ...           ...\n",
      "4540 2015-01-04       15     4240      1299.0           1.0\n",
      "4541 2015-01-04       14    21922        99.0           1.0\n",
      "4542 2015-01-04       15     1969      3999.0           1.0\n",
      "4543 2015-01-04       14    22091       179.0           1.0\n",
      "4544 2015-01-04       15     1007      1199.0           1.0\n",
      "\n",
      "[4545 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def missing_val (raw_sales):\n",
    "    \"\"\"\n",
    "    The function checks each column of the data and performs the following corrections:\n",
    "    Date: inferes the date from the existing values (same date)\n",
    "    Shop_id: replaces NaN for 0\n",
    "    Item_id: replaces NaN for 0\n",
    "    Input: the daily data extracted from the DB.\n",
    "    Output: the data without NaN values\n",
    "    \"\"\"\n",
    "\n",
    "def missing_val (raw_sales):\n",
    "    if raw_sales.isna().sum()['date'] != 0:\n",
    "        existing_values_dates = raw_sales[~(raw_sales.date.isna())]\n",
    "        raw_sales['date'] = raw_sales['date'].fillna(existing_values_dates.loc[0, 'date'])\n",
    "    if raw_sales.isna().sum()['shop_id'] != 0:\n",
    "        raw_sales['shop_id'] = raw_sales['shop_id'].fillna(0)\n",
    "    if raw_sales.isna().sum()['item_id'] != 0:\n",
    "        raw_sales['item_id'] = raw_sales['item_id'].fillna(0)\n",
    "    if raw_sales.isna().sum()['item_price'] != 0:    \n",
    "        records_drop_it_price = raw_sales.isna().sum()['item_price']\n",
    "        percentage_drop_it_price = records_drop_it_price/len(raw_sales)\n",
    "        print('Due to missing values in item_price column, you have', records_drop_it_price, 'record drop. That is a', percentage_drop_it_price, '% of the total dataframe rows.')\n",
    "        raw_sales = raw_sales[~(raw_sales.item_price.isna())]\n",
    "    if raw_sales.isna().sum()['item_cnt_day'] != 0:    \n",
    "        records_drop_item_cnt_day = raw_sales.isna().sum()['item_cnt_day']\n",
    "        percentage_drop_item_cnt_day = records_drop_item_cnt_day/len(raw_sales)\n",
    "        print('Due to missing values in item_cnt_day column, you have', records_drop_it_price, 'record drop. That is a', percentage_drop_it_price, '% of the total dataframe rows.')\n",
    "        raw_sales = raw_sales[~(raw_sales.item_cnt_day.isna())]\n",
    "    return raw_sales\n",
    "clean_raw_sales = missing_val (raw_sales)\n",
    "print(clean_raw_sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type of data:** The defined types seem correct, with the exception of item_cnt_day, that could be stored as an integer assuming that sold quantities can't be decimals (that is, items can't be sold in pieces or parts) However, I would check with our client in that aspect before making any change, as changing the type will cause a loss of information. \n",
    "\n",
    "Nevertheless, if the data contained NaN at some point, the type of the columns will be changed (as an int column can't store a NaN). Therefore, I would create a function that sets the expected types in each column after the NaN correction is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_types = {'date': 'datetime64[ns]', 'shop_id': 'int64', 'item_id': 'int64', 'item_price': 'float64', 'item_cnt_day': 'float64'}\n",
    "for col in raw_sales.columns:\n",
    "    if raw_sales[col].dtypes != dict_types[col]:\n",
    "        raw_sales = raw_sales.astype({col : dict_types[col]})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check data consistency (detect outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           shop_id       item_id    item_price  item_cnt_day\n",
      "count  4545.000000   4545.000000   4545.000000   4545.000000\n",
      "mean     34.021122  11140.459406   1031.686121      1.103630\n",
      "std      16.565517   6558.649572   2073.919990      0.536967\n",
      "min       2.000000     30.000000      3.000000     -1.000000\n",
      "25%      22.000000   4977.000000    249.000000      1.000000\n",
      "50%      31.000000  11247.000000    479.000000      1.000000\n",
      "75%      50.000000  16671.000000   1192.000000      1.000000\n",
      "max      59.000000  22162.000000  27990.000000     10.000000\n"
     ]
    }
   ],
   "source": [
    "print(raw_sales.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantitative description for **shop_id** and **item_id** does not provide usable information as the values in these columns are ids and not quantities.\n",
    "\n",
    "For **item_price** we can see a great dispersion. The distribution is clearly left skewed and the max price seems to be very \n",
    "deviated from the mean. Even when calculating smaller percentiles, the higher percentile shows a great dispersion, indicating outlier(s) in the highest values of the series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest price in the last percentile is  2590.0\n",
      "The highest price in the last percentile is  27990.0\n"
     ]
    }
   ],
   "source": [
    "raw_sales = raw_sales_original\n",
    "percentiles = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "raw_sales['bins'] = pd.qcut(raw_sales['item_price'], len(percentiles), labels = percentiles)\n",
    "# raw_sales[ (raw_sales['bins'] == '10') ].sort_values(by='item_price', ascending = False)\n",
    "print('The lowest price in the last percentile is ', raw_sales[ (raw_sales['bins'] == '10') ].item_price.min())\n",
    "print('The highest price in the last percentile is ', raw_sales[ (raw_sales['bins'] == '10') ].item_price.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to try and understand if that value may be an error, we order the prices and see if the following prices are as high as the first one. Since there seem to be several products with high prices, I would not discard those rows without talking to the client first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904     27990.0\n",
       "981     27392.0\n",
       "400     26990.0\n",
       "983     25392.0\n",
       "1495    19990.0\n",
       "663     14990.0\n",
       "1298     8999.0\n",
       "1152     6990.0\n",
       "1123     6799.0\n",
       "451      5890.0\n",
       "Name: item_price, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_list = raw_sales.item_price\n",
    "prices_list_unique = prices_list.drop_duplicates()\n",
    "prices_list_unique.sort_values(ascending = False, inplace = True)\n",
    "prices_list_unique.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For **item_cnt_day** we observe that the min value is -1. We should check with our client if that means that this item was returned (and therefore the value is correct) or if it is an error. In the case it was indeed an error, and we know for sure what the number stands for (i.e.: it stands for 0 but due to a system error it is stored as -1), we could replace it using the following formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_sales['item_cnt_day'].replace(-1, 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in this case I would prefer not to change the value, as it makes more sense to me to consider -1 as a retunt than considering it a sale of 0 products (which won't actually be a sale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create agregated tables tables for shop view and item view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shop aggregate is: \n",
      "\n",
      "      shop_id  item_id  item_price  item_cnt_day       date\n",
      "0           2     1970     8999.00           1.0 2015-01-04\n",
      "1           2     1971     4499.00           1.0 2015-01-04\n",
      "2           2     2871      999.00           1.0 2015-01-04\n",
      "3           2     2881      999.00           1.0 2015-01-04\n",
      "4           2     3028     2599.00           1.0 2015-01-04\n",
      "...       ...      ...         ...           ...        ...\n",
      "1510       59    20608     1999.00           1.0 2015-01-04\n",
      "1511       59    20949        5.00           2.0 2015-01-04\n",
      "1512       59    21362     1099.00           1.0 2015-01-04\n",
      "1513       59    21364      479.00           1.0 2015-01-04\n",
      "1514       59    21761      772.03           1.0 2015-01-04\n",
      "\n",
      "[1515 rows x 5 columns]\n",
      "\n",
      "The item aggregate is: \n",
      "\n",
      "      item_id  shop_id  item_price  item_cnt_day       date\n",
      "0          30       28       169.0           1.0 2015-01-04\n",
      "1          31        6       363.0           1.0 2015-01-04\n",
      "2          32       31       149.0           1.0 2015-01-04\n",
      "3          42       54       299.0           1.0 2015-01-04\n",
      "4          59       57       249.0           1.0 2015-01-04\n",
      "...       ...      ...         ...           ...        ...\n",
      "1510    22162       21       399.0           1.0 2015-01-04\n",
      "1511    22162       22       399.0           1.0 2015-01-04\n",
      "1512    22162       50       399.0           1.0 2015-01-04\n",
      "1513    22162       56       399.0           1.0 2015-01-04\n",
      "1514    22162       57       399.0           2.0 2015-01-04\n",
      "\n",
      "[1515 rows x 5 columns]\n",
      "\n",
      "The clean sales are: \n",
      "\n",
      "           date  shop_id  item_id  item_price  item_cnt_day\n",
      "0    2015-01-04       29     1469      1199.0           1.0\n",
      "1    2015-01-04       28    21364       479.0           1.0\n",
      "2    2015-01-04       28    21365       999.0           2.0\n",
      "3    2015-01-04       28    22104       249.0           2.0\n",
      "4    2015-01-04       28    22091       179.0           1.0\n",
      "...         ...      ...      ...         ...           ...\n",
      "4540 2015-01-04       15     4240      1299.0           1.0\n",
      "4541 2015-01-04       14    21922        99.0           1.0\n",
      "4542 2015-01-04       15     1969      3999.0           1.0\n",
      "4543 2015-01-04       14    22091       179.0           1.0\n",
      "4544 2015-01-04       15     1007      1199.0           1.0\n",
      "\n",
      "[4545 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "## Shop aggregated view:\n",
    "shop_agg = raw_sales.groupby(['shop_id', 'item_id', 'item_price', 'item_cnt_day', 'date'], as_index = False).count()\n",
    "shop_final = shop_agg.drop(columns='bins')\n",
    "\n",
    "## Item aggregated view:\n",
    "item_agg = raw_sales.groupby(['item_id', 'shop_id', 'item_price', 'item_cnt_day', 'date'], as_index = False).count()\n",
    "item_final = item_agg.drop(columns='bins')\n",
    "\n",
    "##Clean sales view:\n",
    "clean_sales = raw_sales.drop(columns='bins')\n",
    "\n",
    "print('The shop aggregate is: ')\n",
    "print('')\n",
    "print(shop_final)\n",
    "print('')\n",
    "print('The item aggregate is: ')\n",
    "print('')\n",
    "print(item_final)\n",
    "print('')\n",
    "print('The clean sales are: ')\n",
    "print('')\n",
    "print(clean_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shop_final.to_sql('shop_view', con=engine, if_exists='append')\n",
    "# item_final.to_sql('item_view', con=engine, if_exists='append')\n",
    "# clean_sales.to_sql('clean_sales', con=engine, if_exists='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
